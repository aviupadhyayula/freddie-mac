{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389ff058",
   "metadata": {},
   "source": [
    "# 2-keywords-matching (archived)\n",
    "- Note: this notebooks implemented a naive way of matching keywords for both pdfs and website. Since we are not using direct keywords matching for now, this notebook is archived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "def1754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d05e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"minimum lot size\",\n",
    "            \"minimum lot area\",\n",
    "            \"dwelling\", \n",
    "            \"residential\", \n",
    "            \"residence\", \n",
    "            \"residential district\",\n",
    "            \"floor to area ratio\",\n",
    "            \"floor-to-area ratio\",\n",
    "            #\"maximum lot coverage\",\n",
    "            \"side setback\",\n",
    "            \"maximum density\",\n",
    "            \"duplex\",\n",
    "            \"rowhouse\",\n",
    "            #\"condominium\",\n",
    "            \"multifamily\",\n",
    "            \"multi-family\",\n",
    "            \"single family\",\n",
    "            \"single-family\",\n",
    "            \"townhouse\",\n",
    "            \"primary use\",\n",
    "            \"residential type\",\n",
    "            \"minimum parking per unit\",\n",
    "            \"minimum unit size\",\n",
    "            \"public hearing\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f709f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42874b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relevance_scores(text, return_keywords = False):\n",
    "    found_count = 0\n",
    "    keyword_lst = []\n",
    "    for k in keywords:\n",
    "        fc = len(re.findall(k, text))\n",
    "        if fc > 0:\n",
    "            found_count += fc\n",
    "            keyword_lst.append(k)\n",
    "    if return_keywords:\n",
    "        return found_count, keyword_lst\n",
    "    return found_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7d4566-7062-4aec-bca3-eaa8a1f4462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_page(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n-\\n', '-')\n",
    "    return text\n",
    "\n",
    "def pdf_2_text(filepath, k = 1):\n",
    "    pdf_file = open(filepath, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdf_file)\n",
    "    pages = []\n",
    "    for page_idx in tqdm(range(pdfReader.numPages)):\n",
    "        pageObj = pdfReader.getPage(page_idx)\n",
    "        text = pageObj.extractText()\n",
    "        text = process_pdf_page(text)\n",
    "        pages.append(text)\n",
    "    if k == 1:\n",
    "        return pages\n",
    "    agg_pages = []\n",
    "    for p_idx in range(len(pages) - k):\n",
    "        agg_pages.append('\\n\\n'.join(pages[p_idx: p_idx + k - 1]))\n",
    "    return agg_pages\n",
    "\n",
    "def calc_runing_sum(lst, k = 5):\n",
    "    curr_sum = sum(lst[:k])\n",
    "    r_sum = [curr_sum]\n",
    "    for i in range(1, len(lst) - k):\n",
    "        curr_sum -= lst[i - 1]\n",
    "        curr_sum += lst[i + k - 1]\n",
    "        r_sum.append(curr_sum)\n",
    "    return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1e9d3d-7a5b-4fcc-85c4-650b7e12e6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1032/1032 [02:23<00:00,  7.18it/s]\n",
      "100%|███████████████████████████████| 158/158 [00:05<00:00, 29.01it/s]\n",
      "100%|██████████████████████████████| 932/932 [00:05<00:00, 173.68it/s]\n",
      "100%|███████████████████████████████| 311/311 [00:07<00:00, 43.05it/s]\n",
      "100%|███████████████████████████████| 468/468 [00:17<00:00, 26.42it/s]\n"
     ]
    }
   ],
   "source": [
    "file_lst = ['Anne Arundel.pdf', 'Carroll.pdf', 'Calvert.pdf', 'Rockville.pdf', 'Cecil.pdf']\n",
    "page_dict = {}\n",
    "for fp in file_lst:\n",
    "    pages = pdf_2_text('../data/md/' + fp, k = 3)\n",
    "    page_dict[fp] = pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1297a5-b737-44e7-b6b1-dc5d40fb31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for fp in page_dict.keys():\n",
    "    pages = page_dict[fp]\n",
    "    found_counts_dict = {}\n",
    "    for i in range(len(pages)):\n",
    "        found_counts_dict[i] = calc_relevance_scores(pages[i], True)\n",
    "    results[fp] = sorted(found_counts_dict.items(), key = lambda x: x[1][0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4359a43-a93a-476c-9474-1f200f3c23d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(71, 33),\n",
       " (55, 32),\n",
       " (67, 30),\n",
       " (54, 29),\n",
       " (75, 29),\n",
       " (77, 29),\n",
       " (78, 26),\n",
       " (43, 25),\n",
       " (63, 25),\n",
       " (62, 24),\n",
       " (8, 23),\n",
       " (74, 23),\n",
       " (66, 22),\n",
       " (72, 19),\n",
       " (76, 19),\n",
       " (42, 18),\n",
       " (70, 18),\n",
       " (7, 16),\n",
       " (64, 14),\n",
       " (68, 14)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Carroll.pdf'][:20]\n",
    "[(i[0], i[1][0]) for i in results['Carroll.pdf'][:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8541b5ee-0b6d-4f29-8278-0d9141825ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(124, (46, ['minimum lot area', 'dwelling', 'residential', 'duplex'])),\n",
       " (123, (31, ['minimum lot area', 'dwelling', 'residential', 'residence'])),\n",
       " (133, (30, ['dwelling', 'residential', 'duplex', 'townhouse'])),\n",
       " (125, (26, ['dwelling', 'residential', 'duplex'])),\n",
       " (132, (25, ['dwelling', 'residential', 'townhouse'])),\n",
       " (127, (23, ['dwelling', 'residential'])),\n",
       " (27, (21, ['dwelling', 'duplex', 'townhouse'])),\n",
       " (137, (21, ['dwelling', 'residential', 'townhouse'])),\n",
       " (204, (21, ['minimum lot size', 'dwelling', 'residential', 'townhouse'])),\n",
       " (126, (20, ['dwelling', 'residential'])),\n",
       " (186, (20, ['residential', 'townhouse'])),\n",
       " (136, (19, ['dwelling', 'residential', 'townhouse'])),\n",
       " (187, (19, ['dwelling', 'residential', 'townhouse'])),\n",
       " (188, (19, ['dwelling', 'residential', 'townhouse'])),\n",
       " (201, (19, ['residential'])),\n",
       " (225, (19, ['dwelling', 'residential', 'townhouse'])),\n",
       " (226, (18, ['dwelling', 'residential', 'townhouse'])),\n",
       " (131, (17, ['dwelling', 'residential'])),\n",
       " (134, (17, ['dwelling', 'residential', 'duplex', 'townhouse'])),\n",
       " (202, (17, ['residential']))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Rockville.pdf'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d2bf73f-2ddd-49f0-a588-9c2b08135a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(124, 46),\n",
       " (123, 31),\n",
       " (133, 30),\n",
       " (125, 26),\n",
       " (132, 25),\n",
       " (127, 23),\n",
       " (27, 21),\n",
       " (137, 21),\n",
       " (204, 21),\n",
       " (126, 20),\n",
       " (186, 20),\n",
       " (136, 19),\n",
       " (187, 19),\n",
       " (188, 19),\n",
       " (201, 19),\n",
       " (225, 19),\n",
       " (226, 18),\n",
       " (131, 17),\n",
       " (134, 17),\n",
       " (202, 17)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i[0], i[1][0]) for i in results['Rockville.pdf'][:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d7af9-315e-4464-a164-244c27119593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce9627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2666a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d60bdb06",
   "metadata": {},
   "source": [
    "# matching scrapped txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7ccff302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b094c08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'minimum': 3,\n",
       "  'lot size': 2,\n",
       "  'lot area': 2,\n",
       "  'building area': 2,\n",
       "  'acreage': 2,\n",
       "  'acre': 2,\n",
       "  'square feet': 2,\n",
       "  'shall be': 1,\n",
       "  'requirement': 1,\n",
       "  'permitted': 1,\n",
       "  'square feet per dwelling unit': 3},\n",
       " ['lot area',\n",
       "  'lot size',\n",
       "  'building area ',\n",
       "  'building site area acres/dwelling unit',\n",
       "  'square feet/dwelling unit'])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = open('../data/keywords.txt', 'r').readlines()\n",
    "keywords = [w.replace('\\n', '').lower() for w in keywords]\n",
    "sub_keywords = {k[:-4]: int(k[-2]) for k in keywords[5:]}\n",
    "keywords = keywords[:5]\n",
    "\n",
    "sub_keywords_re_pattern = '|'.join(['(' + k + ')' for k in sub_keywords])\n",
    "sub_keywords, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bde244ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('../data/tmp.csv')\n",
    "nums = [0.5, 0.25, 0.75]\n",
    "alphabets = ['half', 'quarter', 'three-quarters']\n",
    "for i in range(1, 6):\n",
    "    numbers = tmp['num' + str(i)].values\n",
    "    words = tmp['alpha' + str(i)].values\n",
    "    nums += list(numbers)\n",
    "    alphabets += list(words)\n",
    "alphabets = [a.lower() for a in alphabets]\n",
    "alphabets_2_numeric = dict(zip(alphabets, nums))\n",
    "alphabets_keys = set(alphabets_2_numeric.keys())\n",
    "# alphabets_keys\n",
    "# alphabets_2_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f8f78d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page_token = '[[START-PAGE]]'\n",
    "start_section_token = '[[SECTIONTITLE]]'\n",
    "start_content_token = '[[CONTENT]]'\n",
    "special_tokens = [start_page_token, start_section_token, start_content_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "97178593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numeric_tokens(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i].strip().lower() in alphabets_keys:\n",
    "            tokens[i] = str(alphabets_2_numeric[tokens[i].strip().lower()])\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746c6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de39103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117cd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "04c513e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 23998/23998 [00:00<00:00, 439354.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 20016/20016 [00:00<00:00, 734620.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 25463/25463 [00:00<00:00, 664102.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 35974/35974 [00:00<00:00, 634854.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 26927/26927 [00:00<00:00, 790315.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 29556/29556 [00:00<00:00, 783355.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_sents_idx: 1\n",
      "Error: no result for neptune.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filepath in os.listdir('../data/scrapped'):\n",
    "    sample_file = '../data/scrapped/' + filepath\n",
    "    f = open(sample_file, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    ########## sentence locations ##########\n",
    "    found_sents_idx = []\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        l = lines[i].lower().strip()\n",
    "        for w in keywords:\n",
    "            if w in l:\n",
    "                tmp_i = i - 1\n",
    "                start = tmp_i\n",
    "                while tmp_i >= 0:\n",
    "                    if lines[tmp_i].strip() in special_tokens:\n",
    "                        start = tmp_i + 1\n",
    "                        break\n",
    "                    tmp_i -= 1\n",
    "                tmp_i = i + 1\n",
    "                end = tmp_i\n",
    "                while tmp_i < len(lines):\n",
    "                    if lines[tmp_i].strip() in special_tokens:\n",
    "                        end = tmp_i\n",
    "                        break\n",
    "                    tmp_i += 1 \n",
    "                if (start, end, w) not in found_sents_idx:\n",
    "                    found_sents_idx.append((start, end, w))\n",
    "    # found_sents_idx = np.unique(found_sents_idx)\n",
    "    print('found_sents_idx:', len(found_sents_idx))\n",
    "    \n",
    "    ########## keyword locations (token level) ##########\n",
    "    keyword_locs = []\n",
    "    for idx_pair in found_sents_idx:\n",
    "        start, end, w = idx_pair\n",
    "        doc = replace_numeric_tokens(' '.join(lines[start: end+1]))\n",
    "        keywords_loc_dict = {}\n",
    "        try:\n",
    "            match_spans = [m.span() for m in re.finditer(w, doc)]\n",
    "        except:\n",
    "            print('ERROR, not found!')\n",
    "            continue\n",
    "        keywords_loc_dict[w] = match_spans\n",
    "\n",
    "        for span in match_spans:\n",
    "            span_start, span_end = span\n",
    "            start = max([span_start - 200, 0])\n",
    "            end = min([span_end + 200, len(doc)])\n",
    "            tmp_doc = doc[start: end + 1]\n",
    "            tokens = word_tokenize(tmp_doc)\n",
    "            numeric_values = []\n",
    "            numeric_idx = -1\n",
    "            for t in tokens:\n",
    "                numeric_idx += 1\n",
    "                try:\n",
    "                    numeric_values.append((float(t), numeric_idx))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            new_span_start = span_start\n",
    "            if start != 0:\n",
    "                new_span_start = 200\n",
    "            span_token_idx = len(word_tokenize(tmp_doc[:new_span_start]))\n",
    "            if len(numeric_values) > 0:\n",
    "                keyword_locs.append([w, tmp_doc, numeric_values, span_token_idx])\n",
    "    \n",
    "    \n",
    "    ########## final results ##########\n",
    "    threshold = 12\n",
    "    selected_docs = []\n",
    "    for k_loc in keyword_locs:\n",
    "        w, doc, loc_lst, span_idx = k_loc\n",
    "        sub_keywords_match = list(re.finditer(sub_keywords_re_pattern, doc))\n",
    "        if len(sub_keywords_match) > 0:\n",
    "            match_score = sum([sub_keywords[m.group()] for m in sub_keywords_match])\n",
    "            if match_score >= threshold:\n",
    "                selected_docs.append((w, doc, loc_lst, span_idx, match_score))\n",
    "\n",
    "    final_results = []\n",
    "    for doc in selected_docs:\n",
    "        w, doc, loc_lst, span_idx, match_score = doc\n",
    "        best_num = []\n",
    "        if span_idx < loc_lst[0][1]:\n",
    "            best_num.append(loc_lst[0][0])\n",
    "        elif span_idx > loc_lst[-1][1]:\n",
    "            best_num.append(loc_lst[-1][0])\n",
    "        else:\n",
    "            for i in range(len(loc_lst) - 1):\n",
    "                if loc_lst[i][1] <= span_idx and loc_lst[i+1][1] >= span_idx:\n",
    "                    best_num.append(loc_lst[i][0])\n",
    "                    best_num.append(loc_lst[i+1][0])\n",
    "        final_results.append({\n",
    "            'keyword': w,\n",
    "            'best_num': best_num,\n",
    "            'doc': doc,\n",
    "            'match_score': match_score\n",
    "        })\n",
    "    if len(final_results) == 0:\n",
    "        print('Error: no result for', filepath)\n",
    "        continue\n",
    "    final_results_df = pd.DataFrame(final_results).sort_values(by = ['keyword', 'match_score'], ascending = False)\n",
    "    final_results_df.to_csv('../data/results/' + filepath.replace('txt', 'csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "87374e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_sents_idx = []\n",
    "\n",
    "# for i in tqdm(range(len(lines))):\n",
    "#     l = lines[i].lower().strip()\n",
    "#     for w in keywords:\n",
    "#         if w in l:\n",
    "#             tmp_i = i - 1\n",
    "#             start = tmp_i\n",
    "#             while tmp_i >= 0:\n",
    "#                 if lines[tmp_i].strip() in special_tokens:\n",
    "#                     start = tmp_i + 1\n",
    "#                     break\n",
    "#                 tmp_i -= 1\n",
    "#             tmp_i = i + 1\n",
    "#             end = tmp_i\n",
    "#             while tmp_i < len(lines):\n",
    "#                 if lines[tmp_i].strip() in special_tokens:\n",
    "#                     end = tmp_i\n",
    "#                     break\n",
    "#                 tmp_i += 1 \n",
    "#             if (start, end, w) not in found_sents_idx:\n",
    "#                 found_sents_idx.append((start, end, w))\n",
    "# # found_sents_idx = np.unique(found_sents_idx)\n",
    "# len(found_sents_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c9dbbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_sents_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b32b6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '23.5'\n",
    "# str.isalnum(s)\n",
    "# # int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f078b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6db0c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8a5a2042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b182d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_locs = []\n",
    "\n",
    "# for idx_pair in found_sents_idx:\n",
    "#     start, end, w = idx_pair\n",
    "#     doc = replace_numeric_tokens(' '.join(lines[start: end+1]))\n",
    "#     keywords_loc_dict = {}\n",
    "#     try:\n",
    "#         match_spans = [m.span() for m in re.finditer(w, doc)]\n",
    "#     except:\n",
    "#         print('ERROR, not found!')\n",
    "#         continue\n",
    "#     keywords_loc_dict[w] = match_spans\n",
    "    \n",
    "#     for span in match_spans:\n",
    "#         span_start, span_end = span\n",
    "#         start = max([span_start - 200, 0])\n",
    "#         end = min([span_end + 200, len(doc)])\n",
    "#         tmp_doc = doc[start: end + 1]\n",
    "#         tokens = word_tokenize(tmp_doc)\n",
    "#         numeric_values = []\n",
    "#         numeric_idx = -1\n",
    "#         for t in tokens:\n",
    "#             numeric_idx += 1\n",
    "#             try:\n",
    "#                 numeric_values.append((float(t), numeric_idx))\n",
    "#             except:\n",
    "#                 continue\n",
    "        \n",
    "#         new_span_start = span_start\n",
    "#         if start != 0:\n",
    "#             new_span_start = 200\n",
    "#         span_token_idx = len(word_tokenize(tmp_doc[:new_span_start]))\n",
    "#         if len(numeric_values) > 0:\n",
    "#             keyword_locs.append([w, tmp_doc, numeric_values, span_token_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c9ba5ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lot area',\n",
       " 'bandoned or ceased for a period of 12 consecutivemonths , it shall be considered abandoned and shall terminate . ( 4 ) To authorize , upon appeal , in specific cases suchvariances from the setback or lot area provisions of this Title aswill not be contrary to the public interest where , owing to specialor unique conditions , a literal enforcement of the provisions of this Title would result in unnecessary ',\n",
       " [(12.0, 7), (4.0, 20)],\n",
       " 36]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_locs)\n",
    "keyword_locs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e9e91467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 12\n",
    "# selected_docs = []\n",
    "# for k_loc in keyword_locs:\n",
    "#     w, doc, loc_lst, span_idx = k_loc\n",
    "#     sub_keywords_match = list(re.finditer(sub_keywords_re_pattern, doc))\n",
    "#     if len(sub_keywords_match) > 0:\n",
    "#         match_score = sum([sub_keywords[m.group()] for m in sub_keywords_match])\n",
    "#         if match_score >= threshold:\n",
    "#             selected_docs.append((w, doc, loc_lst, span_idx, match_score))\n",
    "\n",
    "# final_results = []\n",
    "# for doc in selected_docs:\n",
    "#     w, doc, loc_lst, span_idx, match_score = doc\n",
    "#     best_num = []\n",
    "#     if span_idx < loc_lst[0][1]:\n",
    "#         best_num.append(loc_lst[0][0])\n",
    "#     elif span_idx > loc_lst[-1][1]:\n",
    "#         best_num.append(loc_lst[-1][0])\n",
    "#     else:\n",
    "#         for i in range(len(loc_lst) - 1):\n",
    "#             if loc_lst[i][1] <= span_idx and loc_lst[i+1][1] >= span_idx:\n",
    "#                 best_num.append(loc_lst[i][0])\n",
    "#                 best_num.append(loc_lst[i+1][0])\n",
    "#     final_results.append({\n",
    "#         'keyword': w,\n",
    "#         'best_num': best_num,\n",
    "#         'doc': doc,\n",
    "#         'match_score': match_score\n",
    "#     })\n",
    "# final_results_df = pd.DataFrame(final_results).sort_values(by = ['keyword', 'match_score'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "406de692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_results_df = final_results_df.groupby('keyword').agg('max')\n",
    "final_results_df.to_csv('../data/worcester_result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "351f6974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_num</th>\n",
       "      <th>doc</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>building area</th>\n",
       "      <td>[300.0, 2.0]</td>\n",
       "      <td>feet of an amenity area and 1 per each 5 resid...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot area</th>\n",
       "      <td>[50.0]</td>\n",
       "      <td>unit . ( 1 ) In the E-1 District , 1 unit per ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    best_num  \\\n",
       "keyword                        \n",
       "building area   [300.0, 2.0]   \n",
       "lot area              [50.0]   \n",
       "\n",
       "                                                              doc  match_score  \n",
       "keyword                                                                         \n",
       "building area   feet of an amenity area and 1 per each 5 resid...           20  \n",
       "lot area        unit . ( 1 ) In the E-1 District , 1 unit per ...           18  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_df.groupby('keyword').agg('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647968f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6be50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c753bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7efe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef91b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52342e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb168bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bb6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6eccb-f5d0-4d6d-a98d-68e8a370be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('../data/ordinates_meta_data_sub.csv')\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d82fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28217e11-268f-41d9-a0ff-76de7a8c1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx in range(meta_data.shape[0]):\n",
    "    curr_row = meta_data.iloc[row_idx]\n",
    "    print('=> Processing:', curr_row.county)\n",
    "    f = open('../data/scrapped/' + curr_row.county.replace(' ', '_').lower() + '.txt', 'r')\n",
    "    \n",
    "    f.close()\n",
    "    print(str(ts.root_node)[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6fe49-215f-4a97-94b9-b3bd1b9c13f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
